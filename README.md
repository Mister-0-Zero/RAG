# Локальный RAG-стек для глубокого анализа документов

Этот репозиторий представляет собой production-ориентированный RAG-пайплайн, разработанный для локального развертывания. Ключевая особенность проекта — полный контроль над всеми этапами обработки данных, от инжеста до генерации ответа, без зависимости на внешние облачные сервисы для основной логики.

Проект сфокусирован на работе с локальной базой документов (PDF, DOCX, TXT) в специфическом домене (Human Design), но может быть адаптирован для любых текстовых данных.

## Архитектура пайплайна

Процесс обработки запроса построен как последовательность четко определенных шагов, обеспечивающих максимальную прозрачность и релевантность ответа.

1.  **Анализ запроса**
    *   Определяется язык и категория запроса для возможной дальнейшей маршрутизации или фильтрации.

2.  **Гибридный поиск (Hybrid Retrieval)**
    *   **Dense Retrieval**: Векторный поиск по семантической близости с использованием `SentenceTransformers` и `ChromaDB`.
    *   **Lexical Retrieval**: Поиск по ключевым словам (BM25) с помощью `Elasticsearch`.
    *   Результаты обоих поисков объединяются и нормализуются для получения единого списка кандидатов.

3.  **Переранжирование (Reranking)**
    *   Полученные кандидаты проходят через `cross-encoder` модель, которая более точно оценивает релевантность каждого чанка непосредственно к запросу. Это позволяет значительно повысить качество финального контекста.

4.  **Расширение контекста (Context Expansion)**
    *   Для лучших `k` чанков, отобранных после переранжирования, выполняется поиск соседних чанков (`neighbors`). Это делается через прямой `metadata lookup` по их идентификаторам и смещениям, что позволяет восстановить окружающий контекст без дополнительного семантического поиска.

5.  **Сжатие контекста (Context Compression)**
    *   Каждый результирующий фрагмент (основной чанк + его соседи) сжимается с помощью малой локальной LLM. Цель — удалить "шум" и оставить только ту информацию, которая напрямую относится к запросу, делая финальный контекст более компактным и сфокусированным.

6.  **Генерация ответа (Answering)**
    *   Сжатый и обогащенный контекст передается в основную LLM (локальную или через API) для синтеза финального ответа. Система настроена так, чтобы отвечать строго на основе предоставленного контекста и указывать источники.

## Структура проекта и модули

Проект имеет модульную структуру, где каждый компонент отвечает за свою часть пайплайна.

-   `ingest.py`: Отвечает за загрузку сырых документов (`.pdf`, `.docx`, `.txt`) из директории `data/raw`, их очистку и базовую нормализацию текста.

-   `chunking.py`: Производит разбиение очищенного текста на чанки. Контролирует создание и сохранение метаданных для каждого чанка (ID документа, имя, порядок, смещения символов `start_char`, `end_char`).

-   `embeddings.py`: Управляет загрузкой и использованием embedding-моделей (`SentenceTransformers`) для преобразования текстовых чанков в векторы.

-   `vector_store.py`: Абстракция над `ChromaDB`. Отвечает исключительно за хранение векторов и выполнение семантического поиска.

-   `lexical_es.py`: Управляет взаимодействием с `Elasticsearch` для выполнения lexical/BM25 поиска. Восстанавливает полные данные чанка из `_source`.

-   `hybrid.py`: Реализует логику объединения результатов dense и lexical поиска, включая нормализацию скоров.

-   `rerank.py`: Содержит логику для переранжирования с помощью cross-encoder моделей. Учитывает доступность устройства (CPU/GPU).

-   `compressor.py`: Обертка для малой локальной LLM, которая используется исключительно для сжатия контекста перед подачей в основную модель.

-   `pipeline.py`: Собирает все компоненты в единый end-to-end пайплайн, оркестрируя вызовы от получения запроса до генерации ответа.

-   `cli/main.py`: Точка входа для взаимодействия с пользователем через командную строку.

-   `search/es_client.py`: Клиент для подключения к `Elasticsearch`.

-   `support_function/detect_function.py`: Вспомогательные функции, например, для детекции языка.

-   `logger.py`: Конфигурация кастомного логгера для структурированного и цветного логирования этапов пайплайна.

## Установка

1.  **Клонируйте репозиторий:**
    ```bash
    git clone <your-repo-url>
    cd <repo-name>
    ```

2.  **Установите зависимости:**
    ```bash
    pip install -r requirements.txt
    ```
    > Обратите дополнительное внимание к torch, необходимо загрузить версию, ориентированную под ваши CUDA, иначе вычисления будут производиться на CPU.

3.  **Запустите сервисы:**
    Проект требует `Elasticsearch` и `ChromaDB`. Для их быстрого запуска используйте Docker.
    ```bash
    docker-compose up -d
    ```

4.  **Загрузка локальной модели**:
    > Установите сервис Ollama с официального сайта, если он у вас не установлен.
    > ```
    > https://ollama.com/
    > ```
    > Затем установите модель. В проекте используется `qwen2:1.5b-instruct`. Вы можете использовать и другую модель, но рекомендуется выбрать не очень большую, так как её задача — очищать контекст, а не генерировать ответ.
    > 
    > Загрузка модели:
    > ```bash
    > ollama pull qwen2:1.5b-instruct
    > ```

## Использование

### Первый запуск (индексация данных)

При первом запуске или при обновлении документов в `data/raw` необходимо запустить процесс индексации. Он включает в себя загрузку, обработку и сохранение данных в `ChromaDB` и `Elasticsearch`.

Используйте флаг `--reindex`:

```bash
python -m cli.main --reindex
```

> Этот процесс может занять некоторое время в зависимости от объема данных.

### Последующие запуски (выполнение запросов)

После того как данные проиндексированы, вы можете выполнять запросы к базе документов.

```bash
python -m cli.main "Ваш вопрос к базе документов"
```
